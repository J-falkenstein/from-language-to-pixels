As a text-based AI assistant, I'm not able to directly create or display images. However, I can explain to you how this would look like.

A pixel matrix for a simple image of crab may consist of multiple colors (represented by RGB values) to represent different parts of the crab. For instance, red might be used for the body and claws (255, 0, 0), blue for the eyes (0, 0, 255), and perhaps a sandy color for the beach on the background (244, 164, 96).

In reality, a simple image like this would still require a large matrix of pixels to represent, as even a small 100x100 pixel image still contains 10,000 individual pixels. For a more complex and detailed image, the matrix will be larger.

Here's a very simplified conceptual representation of such an image:

[
[(244, 164, 96), (244, 164, 96), (244, 164, 96), (244, 164, 96), (255, 0, 0)],
[(244, 164, 96), (0, 0, 255), (255, 0, 0), (0, 0, 255), (244, 164, 96)],
[(244, 164, 96), (255, 0, 0), (255, 0, 0), (255, 0, 0), (244, 164, 96)],
[(244, 164, 96), (244, 164, 96), (255, 0, 0), (244, 164, 96), (244, 164, 96)],
[(244, 164, 96), (244, 164, 96), (244, 164, 96), (244, 164, 96), (244, 164, 96)]
]

In this example, the crab's eyes are blue pixels in the middle of the red row, and the rest of the crab body is represented by the red pixels. The sandy background is represented by the RGB value (244, 164, 96) and is the majority of the image. Of course, this is a very simplified representation and actual image of a crab would be much more detailed and larger in size.